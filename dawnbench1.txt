PS C:\Users\Nebula PC\Documents\COMP3710\DEMO2> python 33.py --lr 0.3 --weight-decay 2e-4
C:\Users\Nebula PC\Documents\COMP3710\DEMO2\33.py:198: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
C:\Users\Nebula PC\Documents\COMP3710\DEMO2\33.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
Epoch 1/90  loss=2.1671  val_acc=40.77%  epoch_time=69.5s
Epoch 2/90  loss=2.0520  val_acc=45.30%  epoch_time=39.1s
Epoch 3/90  loss=1.9674  val_acc=53.79%  epoch_time=39.2s
Epoch 4/90  loss=1.9233  val_acc=58.29%  epoch_time=39.3s
Epoch 5/90  loss=1.8934  val_acc=60.92%  epoch_time=41.8s
Epoch 6/90  loss=1.8115  val_acc=62.82%  epoch_time=39.4s
Epoch 7/90  loss=1.7804  val_acc=66.67%  epoch_time=38.8s
Epoch 8/90  loss=1.7314  val_acc=67.81%  epoch_time=39.0s
Epoch 9/90  loss=1.7209  val_acc=71.34%  epoch_time=39.0s
Epoch 10/90  loss=1.7224  val_acc=72.55%  epoch_time=41.5s
Epoch 11/90  loss=1.7224  val_acc=72.52%  epoch_time=39.7s
Epoch 12/90  loss=1.6731  val_acc=73.04%  epoch_time=39.8s
Epoch 13/90  loss=1.6369  val_acc=76.69%  epoch_time=40.1s
Epoch 14/90  loss=1.6081  val_acc=76.56%  epoch_time=40.0s
Epoch 15/90  loss=1.6095  val_acc=77.93%  epoch_time=39.6s
Epoch 16/90  loss=1.6557  val_acc=78.40%  epoch_time=39.9s
Epoch 17/90  loss=1.6011  val_acc=78.42%  epoch_time=39.7s
Epoch 18/90  loss=1.5949  val_acc=80.25%  epoch_time=39.5s
Epoch 19/90  loss=1.5534  val_acc=80.72%  epoch_time=40.9s
Epoch 20/90  loss=1.5413  val_acc=80.29%  epoch_time=40.5s
Epoch 21/90  loss=1.5539  val_acc=81.57%  epoch_time=42.2s
Epoch 22/90  loss=1.5800  val_acc=81.02%  epoch_time=39.3s
Epoch 23/90  loss=1.4774  val_acc=83.03%  epoch_time=38.9s
Epoch 24/90  loss=1.4881  val_acc=80.71%  epoch_time=38.7s
Epoch 25/90  loss=1.5419  val_acc=82.89%  epoch_time=38.4s
Epoch 26/90  loss=1.5252  val_acc=83.64%  epoch_time=39.0s
Epoch 27/90  loss=1.5028  val_acc=81.90%  epoch_time=39.8s
Epoch 28/90  loss=1.4553  val_acc=84.24%  epoch_time=38.4s
Epoch 29/90  loss=1.4675  val_acc=84.07%  epoch_time=38.7s
Epoch 30/90  loss=1.4691  val_acc=82.50%  epoch_time=38.6s
Epoch 31/90  loss=1.4678  val_acc=83.61%  epoch_time=38.7s
Epoch 32/90  loss=1.4783  val_acc=85.57%  epoch_time=39.1s
Epoch 33/90  loss=1.4979  val_acc=85.68%  epoch_time=38.5s
Epoch 34/90  loss=1.4721  val_acc=86.22%  epoch_time=38.7s
Epoch 35/90  loss=1.4327  val_acc=85.44%  epoch_time=38.5s
Epoch 36/90  loss=1.4716  val_acc=85.37%  epoch_time=38.6s
Epoch 37/90  loss=1.4541  val_acc=87.10%  epoch_time=38.4s
Epoch 38/90  loss=1.4395  val_acc=85.05%  epoch_time=38.7s
Epoch 39/90  loss=1.4863  val_acc=87.20%  epoch_time=38.6s
Epoch 40/90  loss=1.4238  val_acc=86.91%  epoch_time=38.5s
Epoch 41/90  loss=1.4173  val_acc=86.59%  epoch_time=38.6s
Epoch 42/90  loss=1.4314  val_acc=85.89%  epoch_time=38.4s
Epoch 43/90  loss=1.4599  val_acc=84.21%  epoch_time=38.4s
Epoch 44/90  loss=1.4567  val_acc=86.17%  epoch_time=38.4s
Epoch 45/90  loss=1.3899  val_acc=86.96%  epoch_time=38.5s
Epoch 46/90  loss=1.4492  val_acc=85.35%  epoch_time=38.9s
Epoch 47/90  loss=1.3948  val_acc=87.34%  epoch_time=38.6s
Epoch 48/90  loss=1.3848  val_acc=86.66%  epoch_time=39.7s
Epoch 49/90  loss=1.4603  val_acc=88.20%  epoch_time=38.8s
Epoch 50/90  loss=1.4059  val_acc=87.36%  epoch_time=38.9s
Epoch 51/90  loss=1.4338  val_acc=88.56%  epoch_time=38.9s
Epoch 52/90  loss=1.3905  val_acc=87.86%  epoch_time=38.8s
Epoch 53/90  loss=1.3852  val_acc=89.24%  epoch_time=39.0s
Epoch 54/90  loss=1.4173  val_acc=89.31%  epoch_time=38.9s
Epoch 55/90  loss=1.4005  val_acc=88.32%  epoch_time=39.0s
Epoch 56/90  loss=1.3946  val_acc=89.32%  epoch_time=38.7s
Epoch 57/90  loss=1.3986  val_acc=88.28%  epoch_time=38.6s
Epoch 58/90  loss=1.3789  val_acc=89.20%  epoch_time=38.3s
Epoch 59/90  loss=1.3683  val_acc=89.34%  epoch_time=38.7s
Epoch 60/90  loss=1.3718  val_acc=88.75%  epoch_time=38.4s
Epoch 61/90  loss=1.3772  val_acc=89.29%  epoch_time=38.5s
Epoch 62/90  loss=1.3833  val_acc=88.53%  epoch_time=38.8s
Epoch 63/90  loss=1.4292  val_acc=88.28%  epoch_time=38.6s
Epoch 64/90  loss=1.3613  val_acc=89.43%  epoch_time=38.7s
Epoch 65/90  loss=1.3668  val_acc=88.06%  epoch_time=38.8s
Epoch 66/90  loss=1.2814  val_acc=89.05%  epoch_time=38.9s
Epoch 67/90  loss=1.3409  val_acc=89.77%  epoch_time=38.7s
Epoch 68/90  loss=1.3613  val_acc=88.80%  epoch_time=38.8s
Epoch 69/90  loss=1.3444  val_acc=90.24%  epoch_time=38.6s
Epoch 70/90  loss=1.3955  val_acc=90.34%  epoch_time=38.5s
Epoch 71/90  loss=1.3955  val_acc=89.64%  epoch_time=38.5s
Epoch 72/90  loss=1.3420  val_acc=90.27%  epoch_time=38.6s
Epoch 73/90  loss=1.3340  val_acc=90.72%  epoch_time=39.1s
Epoch 74/90  loss=1.3271  val_acc=86.94%  epoch_time=38.5s
Epoch 75/90  loss=1.3892  val_acc=89.06%  epoch_time=38.7s
Epoch 76/90  loss=1.3187  val_acc=90.14%  epoch_time=38.5s
Epoch 77/90  loss=1.3961  val_acc=90.33%  epoch_time=38.4s
Epoch 78/90  loss=1.3812  val_acc=90.17%  epoch_time=38.7s
Epoch 79/90  loss=1.3413  val_acc=90.02%  epoch_time=38.5s
Epoch 80/90  loss=1.3850  val_acc=90.92%  epoch_time=38.7s
Epoch 81/90  loss=1.3253  val_acc=90.27%  epoch_time=38.5s
Epoch 82/90  loss=1.3142  val_acc=90.14%  epoch_time=38.5s
Epoch 83/90  loss=1.3570  val_acc=90.76%  epoch_time=38.5s
Epoch 84/90  loss=1.2685  val_acc=91.16%  epoch_time=38.8s
Epoch 85/90  loss=1.3977  val_acc=90.30%  epoch_time=38.8s
Epoch 86/90  loss=1.3490  val_acc=90.47%  epoch_time=38.5s
Epoch 87/90  loss=1.3154  val_acc=90.16%  epoch_time=38.6s
Epoch 88/90  loss=1.3159  val_acc=90.26%  epoch_time=38.7s
Epoch 89/90  loss=1.2916  val_acc=91.44%  epoch_time=39.1s
Epoch 90/90  loss=1.2881  val_acc=91.63%  epoch_time=38.5s

PS C:\Users\Nebula PC\Documents\COMP3710\DEMO2> python 33.py --lr 0.47 --weight-decay 2e-4
C:\Users\Nebula PC\Documents\COMP3710\DEMO2\33.py:198: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)
C:\Users\Nebula PC\Documents\COMP3710\DEMO2\33.py:146: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=args.amp):
Epoch 1/90  loss=2.1620  val_acc=38.70%  epoch_time=67.7s
Epoch 2/90  loss=2.0064  val_acc=49.90%  epoch_time=38.2s
Epoch 3/90  loss=1.9147  val_acc=57.01%  epoch_time=38.2s
Epoch 4/90  loss=1.8419  val_acc=62.67%  epoch_time=38.4s
Epoch 5/90  loss=1.7830  val_acc=66.96%  epoch_time=38.8s
Epoch 6/90  loss=1.7572  val_acc=64.78%  epoch_time=38.6s
Epoch 7/90  loss=1.6926  val_acc=71.64%  epoch_time=38.4s
Epoch 8/90  loss=1.6558  val_acc=74.04%  epoch_time=38.6s
Epoch 9/90  loss=1.6498  val_acc=74.75%  epoch_time=38.3s
Epoch 10/90  loss=1.6108  val_acc=77.71%  epoch_time=39.0s
Epoch 11/90  loss=1.6029  val_acc=76.41%  epoch_time=39.0s
Epoch 12/90  loss=1.6000  val_acc=76.83%  epoch_time=38.9s
Epoch 13/90  loss=1.5997  val_acc=79.83%  epoch_time=38.7s
Epoch 14/90  loss=1.5455  val_acc=80.74%  epoch_time=38.8s
Epoch 15/90  loss=1.5589  val_acc=80.99%  epoch_time=38.8s
Epoch 16/90  loss=1.5321  val_acc=81.20%  epoch_time=38.8s
Epoch 17/90  loss=1.5472  val_acc=81.49%  epoch_time=38.6s
Epoch 18/90  loss=1.4803  val_acc=82.89%  epoch_time=38.8s
Epoch 19/90  loss=1.5328  val_acc=82.30%  epoch_time=38.9s
Epoch 20/90  loss=1.5489  val_acc=83.57%  epoch_time=38.7s
Epoch 21/90  loss=1.5355  val_acc=83.03%  epoch_time=38.6s
Epoch 22/90  loss=1.4743  val_acc=83.04%  epoch_time=38.7s
Epoch 23/90  loss=1.5348  val_acc=79.48%  epoch_time=38.7s
Epoch 24/90  loss=1.5104  val_acc=83.04%  epoch_time=38.9s
Epoch 25/90  loss=1.4849  val_acc=85.60%  epoch_time=38.7s
Epoch 26/90  loss=1.4488  val_acc=85.15%  epoch_time=38.8s
Epoch 27/90  loss=1.4421  val_acc=86.25%  epoch_time=38.7s
Epoch 28/90  loss=1.4465  val_acc=83.81%  epoch_time=38.6s
Epoch 29/90  loss=1.4477  val_acc=86.23%  epoch_time=38.7s
Epoch 30/90  loss=1.4274  val_acc=86.93%  epoch_time=38.5s
Epoch 31/90  loss=1.4159  val_acc=84.80%  epoch_time=38.6s
Epoch 32/90  loss=1.4266  val_acc=86.37%  epoch_time=38.7s
Epoch 33/90  loss=1.4494  val_acc=84.48%  epoch_time=38.7s
Epoch 34/90  loss=1.4054  val_acc=86.70%  epoch_time=39.4s
Epoch 35/90  loss=1.4001  val_acc=88.13%  epoch_time=38.7s
Epoch 36/90  loss=1.3658  val_acc=88.37%  epoch_time=38.6s
Epoch 37/90  loss=1.4487  val_acc=84.78%  epoch_time=38.6s
Epoch 38/90  loss=1.3877  val_acc=87.92%  epoch_time=38.7s
Epoch 39/90  loss=1.4108  val_acc=88.58%  epoch_time=38.9s
Epoch 40/90  loss=1.4226  val_acc=86.94%  epoch_time=38.8s
Epoch 41/90  loss=1.3349  val_acc=88.03%  epoch_time=38.7s
Epoch 42/90  loss=1.4073  val_acc=89.25%  epoch_time=38.5s
Epoch 43/90  loss=1.4345  val_acc=88.82%  epoch_time=38.8s
Epoch 44/90  loss=1.3798  val_acc=87.53%  epoch_time=38.7s
Epoch 45/90  loss=1.3745  val_acc=89.52%  epoch_time=38.5s
Epoch 46/90  loss=1.3815  val_acc=89.19%  epoch_time=38.6s
Epoch 47/90  loss=1.3875  val_acc=88.74%  epoch_time=38.5s
Epoch 48/90  loss=1.3749  val_acc=89.95%  epoch_time=38.7s
Epoch 49/90  loss=1.3485  val_acc=86.88%  epoch_time=38.8s
Epoch 50/90  loss=1.3772  val_acc=89.43%  epoch_time=38.7s
Epoch 51/90  loss=1.3309  val_acc=89.86%  epoch_time=38.8s
Epoch 52/90  loss=1.3819  val_acc=88.33%  epoch_time=38.7s
Epoch 53/90  loss=1.4070  val_acc=89.54%  epoch_time=38.7s
Epoch 54/90  loss=1.3668  val_acc=89.31%  epoch_time=38.7s
Epoch 55/90  loss=1.3498  val_acc=88.73%  epoch_time=38.7s
Epoch 56/90  loss=1.3240  val_acc=89.75%  epoch_time=38.7s
Epoch 57/90  loss=1.4203  val_acc=89.74%  epoch_time=38.7s
Epoch 58/90  loss=1.3405  val_acc=89.84%  epoch_time=38.5s
Epoch 59/90  loss=1.3188  val_acc=90.61%  epoch_time=38.7s
Epoch 60/90  loss=1.3211  val_acc=90.18%  epoch_time=38.6s
Epoch 61/90  loss=1.3696  val_acc=90.90%  epoch_time=38.7s
Epoch 62/90  loss=1.3835  val_acc=89.50%  epoch_time=38.6s
Epoch 63/90  loss=1.3050  val_acc=89.26%  epoch_time=38.9s
Epoch 64/90  loss=1.3532  val_acc=89.23%  epoch_time=38.7s
Epoch 65/90  loss=1.3308  val_acc=90.26%  epoch_time=38.7s
Epoch 66/90  loss=1.3393  val_acc=90.87%  epoch_time=38.7s
Epoch 67/90  loss=1.3519  val_acc=91.35%  epoch_time=38.7s
Epoch 68/90  loss=1.3556  val_acc=90.17%  epoch_time=38.8s
Epoch 69/90  loss=1.2849  val_acc=90.40%  epoch_time=38.7s
Epoch 70/90  loss=1.3598  val_acc=90.39%  epoch_time=38.7s
Epoch 71/90  loss=1.3621  val_acc=90.95%  epoch_time=38.5s
Epoch 72/90  loss=1.2887  val_acc=91.88%  epoch_time=38.7s
Epoch 73/90  loss=1.3046  val_acc=89.71%  epoch_time=38.8s
Epoch 74/90  loss=1.3406  val_acc=91.59%  epoch_time=38.5s
Epoch 75/90  loss=1.3137  val_acc=91.16%  epoch_time=38.7s
Epoch 76/90  loss=1.2984  val_acc=89.97%  epoch_time=38.7s
Epoch 77/90  loss=1.2773  val_acc=91.18%  epoch_time=38.9s
Epoch 78/90  loss=1.2980  val_acc=90.11%  epoch_time=39.1s
Epoch 79/90  loss=1.3117  val_acc=89.98%  epoch_time=38.9s
Epoch 80/90  loss=1.2759  val_acc=91.39%  epoch_time=38.7s
Epoch 81/90  loss=1.3342  val_acc=89.98%  epoch_time=38.7s
Epoch 82/90  loss=1.2710  val_acc=90.27%  epoch_time=38.9s
Epoch 83/90  loss=1.2885  val_acc=91.42%  epoch_time=38.9s
Epoch 84/90  loss=1.2861  val_acc=91.15%  epoch_time=38.8s
Epoch 85/90  loss=1.2556  val_acc=92.04%  epoch_time=38.9s
Epoch 86/90  loss=1.2625  val_acc=91.10%  epoch_time=39.0s
Epoch 87/90  loss=1.3093  val_acc=92.12%  epoch_time=38.7s
Epoch 88/90  loss=1.2614  val_acc=91.21%  epoch_time=38.8s
Epoch 89/90  loss=1.2998  val_acc=92.20%  epoch_time=38.7s
Epoch 90/90  loss=1.3330  val_acc=91.79%  epoch_time=38.8s